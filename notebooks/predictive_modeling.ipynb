{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b3570518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, auc, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import  OneHotEncoder, scale, StandardScaler\n",
    "from sklearn.compose import  ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pf = pd.read_csv(\"/Users/hayden/Downloads/Projects/used_car_project/data/cleaned/used_cars_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7572943",
   "metadata": {},
   "source": [
    "# Feature and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "accdf1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3868, 13)\n",
      "(3868,)\n"
     ]
    }
   ],
   "source": [
    "X = pf.drop('price', axis = 1)\n",
    "y = pf['price']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7a965",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0e097706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3094, 13) (3094,)\n",
      "(774, 13) (774,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1febe93",
   "metadata": {},
   "source": [
    "# Handle string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "692f7964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric:  ['mileage', 'accident', 'clean_title', 'car_age', 'price_per_mile']\n",
      "Categoric:  ['brand', 'model', 'fuel_type', 'engine', 'ext_col', 'int_col', 'mile_bin', 'ext_col_grouped']\n"
     ]
    }
   ],
   "source": [
    "num_col = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_col = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric: ', num_col)\n",
    "print('Categoric: ', cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4cc06",
   "metadata": {},
   "source": [
    "# Differentiate numerical columns and categorical columns\n",
    "    Since the models can not interpret categorical values such as toyota, subaru, we need to differentiate numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e72eb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[('num', 'passthrough', num_col), ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e75668",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "55de35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6743167151860241\n",
      "RMSE: 21330.945926332144\n",
      "MAE: 30136.387037690285\n"
     ]
    }
   ],
   "source": [
    "LinReg = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])\n",
    "LinReg.fit(X_train, y_train)\n",
    "pred_LinReg = LinReg.predict(X_test)\n",
    "\n",
    "print('R^2:', r2_score(y_test, pred_LinReg))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred_LinReg)))\n",
    "print('MAE:', mean_absolute_error(y_test, pred_rid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a8950",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2e2e8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.3552548934186752\n",
      "RMSE: 30012.79261267522\n",
      "MAE: 18338.083534536076\n"
     ]
    }
   ],
   "source": [
    "ridge = Pipeline([('preprocessor', preprocessor), ('model', Ridge(alpha=4))])\n",
    "ridge.fit(X_train, y_train)\n",
    "pred_rid = ridge.predict(X_test)\n",
    "\n",
    "print('R^2:', r2_score(y_test, pred_rid))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred_rid)))\n",
    "print('MAE:', mean_absolute_error(y_test, pred_rid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a94b87",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d3624a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.8067421059143987\n",
      "RMSE: 16431.643401836944\n",
      "MAE: 9071.290710130868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hayden/Downloads/Projects/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:656: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.328e+10, tolerance: 4.247e+08\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lasso = Pipeline([('preprocessor', preprocessor), ('model', Lasso(alpha=0.01, max_iter=10000))])\n",
    "lasso.fit(X_train, y_train)\n",
    "pred_las = lasso.predict(X_test)\n",
    "\n",
    "print('R^2:', r2_score(y_test, pred_las))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred_las)))\n",
    "print('MAE:', mean_absolute_error(y_test, pred_las))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9edffe",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "    Better than linear/ridge/lasso with handling nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2fec8a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7121253276710504\n",
      "RMSE: 20054.60480755181\n",
      "MAE: 9106.237726098192\n"
     ]
    }
   ],
   "source": [
    "ran_for = Pipeline([('preprocessor', preprocessor), ('model', RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1))])\n",
    "ran_for.fit(X_train, y_train)\n",
    "pred_ran_for = ran_for.predict(X_test)\n",
    "\n",
    "print('R^2:', r2_score(y_test, pred_ran_for))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred_ran_for)))\n",
    "print('MAE:', mean_absolute_error(y_test, pred_ran_for))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22e338",
   "metadata": {},
   "source": [
    "# GradBoost\n",
    "    handles skewed value well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cc8faf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.963482006447604\n",
      "RMSE: 7142.756313927941\n",
      "MAE: 3423.8317932260447\n"
     ]
    }
   ],
   "source": [
    "gb = Pipeline([('preprocessor', preprocessor), ('model', GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42))])\n",
    "gb.fit(X_train, y_train)\n",
    "pred_gb = gb.predict(X_test)\n",
    "\n",
    "print('R^2:', r2_score(y_test, pred_gb))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred_gb)))\n",
    "print('MAE:', mean_absolute_error(y_test, pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6bd8f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost or LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdc64c",
   "metadata": {},
   "source": [
    "# Comparisons\n",
    "\n",
    "### R^2\n",
    "    R^2 shows how much variation of the target the model explains\n",
    "    Therefore higher R^2 value that covers more variation is the better than the lower R^2 value\n",
    "### MAE\n",
    "    MAE gives average value of the difference between predicted and actual prices.\n",
    "    Therefore smaller the MAE is, the smaller the error between predicted and actual prices is\n",
    "### RMSE\n",
    "    RMSE is simliar to MAE but RMSE squares the error value before calculation the average and takes the square root from the resulted average\n",
    "    Meaning, RMSE penalizes larger errors heavily.\n",
    "    Therefore, the smaller the RMSE is, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b948f9",
   "metadata": {},
   "source": [
    "# Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7de861e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model   R^2      RMSE           MAE\n",
      "0  Linear Regression  0.63  23160.49  2.071048e+09\n",
      "1              Ridge  0.28  33622.45  1.130469e+09\n",
      "2              Lasso  0.78  18561.23  3.445192e+09\n",
      "3      Random Forest  0.70  21533.38  4.636864e+08\n",
      "4  Gradient Boosting  0.97   7175.24  5.148412e+07\n"
     ]
    }
   ],
   "source": [
    "comparisons = {\"Model\": [ 'Linear Regression', 'Ridge', 'Lasso', 'Random Forest', 'Gradient Boosting'],\n",
    "               'R^2': [0.63, 0.28 , 0.78, 0.70, 0.97],\n",
    "               'RMSE' : [23160.49, 33622.45, 18561.23, 21533.38, 7175.24],\n",
    "               'MAE': [2071048268.65, 1130469424.44 , 3445192004.44, 463686401.87, 51484124.65],\n",
    "               }\n",
    "\n",
    "df_comp = pd.DataFrame(comparisons)\n",
    "\n",
    "print(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828ebc7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- 5 predictive model were implementd and compared using R^2, RMSE, and MAE.\n",
    "- Gradient Boosting model dominated other 4 models in all aspects by having highest R^2 and lowest RMSE and MAE.\n",
    "- This shows the Gradient Boosting is the most effective model to predict used car prices for this dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual env",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
